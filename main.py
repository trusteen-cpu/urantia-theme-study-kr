import streamlit as st
import os
import re
from openai import OpenAI

# -----------------------
# ê¸°ë³¸ ì„¤ì •
# -----------------------
st.set_page_config(page_title="ìœ ë€ì‹œì•„ ì£¼ì œ ì—°êµ¬ â€“ Korean", layout="wide")
st.title("ğŸ“˜ ìœ ë€ì‹œì•„ ì£¼ì œ ì—°êµ¬ â€“ Korean Edition")
st.caption("í•œê¸€ ìœ ë€ì‹œì•„ì„œ ë³¸ë¬¸ì—ì„œ ì£¼ì œë¥¼ ì°¾ì•„ AIê°€ ë³´ê³ ì„œì™€ ìŠ¬ë¼ì´ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")

# -----------------------
# API Key (Render í™˜ê²½ ë³€ìˆ˜)
# -----------------------
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    st.error("âš ï¸ OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. Render í™˜ê²½ ë³€ìˆ˜ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    st.stop()

# -----------------------
# íŒŒì¼ ê²½ë¡œ ì„¤ì •
# -----------------------
DATA_DIR = "data"
CANDIDATE_FILES = ["urantia_ko.txt", "urantia_kr.txt", "urantia.txt"]

def find_existing_path():
    for name in CANDIDATE_FILES:
        path = os.path.join(DATA_DIR, name)
        if os.path.exists(path):
            return path
    return None

KR_PATH = find_existing_path()

# -----------------------
# íŒŒì¼ ì½ê¸°
# -----------------------
def safe_read_text(path: str) -> list[str]:
    encodings = ["utf-8", "utf-8-sig", "cp949", "euc-kr", "latin-1"]
    for enc in encodings:
        try:
            with open(path, "r", encoding=enc) as f:
                lines = f.readlines()
                cleaned = [l.replace("\ufeff", "").rstrip("\r\n") for l in lines if l.strip()]
                return cleaned
        except Exception:
            continue
    return []

@st.cache_data
def load_urantia_kr():
    if not KR_PATH:
        return []
    return safe_read_text(KR_PATH)

urantia_lines = load_urantia_kr()

# -----------------------
# ê²€ìƒ‰ í•¨ìˆ˜
# -----------------------
def search_passages(keyword: str, lines: list[str], limit: int = 200):
    if not keyword:
        return []
    key = keyword.strip()
    try:
        pattern = re.compile(re.escape(key))
    except re.error:
        pattern = re.compile(key)

    results = []
    for i, line in enumerate(lines, 1):
        clean_line = line.replace("\ufeff", "")
        if re.search(pattern, clean_line):
            results.append(f"{i}: {clean_line}")
    return results[:limit]

# -----------------------
# GPT ë³´ê³ ì„œ ìƒì„±
# -----------------------
def generate_gpt_report_and_slides(term: str, passages: list[str]):
    client = OpenAI(api_key=api_key)
    joined_passages = "\n".join(passages) or "ê´€ë ¨ êµ¬ì ˆì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    # ì‚¼ì¤‘ ë”°ì˜´í‘œ ì œê±°í•œ ì•ˆì „í•œ ë¬¸ìì—´ (Render í˜¸í™˜)
    prompt = (
        "ë‹¹ì‹ ì€ ìœ ë€ì‹œì•„ì„œë¥¼ ì—°êµ¬í•˜ëŠ” ì‹ í•™ìì…ë‹ˆë‹¤.\n\n"
        f"ì£¼ì œ: {term}\n\n"
        "ì•„ë˜ëŠ” ì´ ì£¼ì œì™€ ê´€ë ¨ ìˆë‹¤ê³  íŒë‹¨ë˜ëŠ” ìœ ë€ì‹œì•„ì„œ ë³¸ë¬¸ì…ë‹ˆë‹¤.\n\n"
        "---\n\n"
        "1ë¶€. ì‹ í•™ì  ë³´ê³ ì„œ (700~1000ì)\n"
        "- ì´ ì£¼ì œì˜ ìœ ë€ì‹œì•„ì  ì˜ë¯¸\n"
        "- ì‹ ì„±/ìš°ì£¼ë¡ ì  ì¤‘ìš”ì„±\n"
        "- ì•„ë²„ì§€, ìµœê·¹ì¡´ì¬, ìƒê°ì¡°ìœ¨ìì™€ì˜ ê´€ê³„\n"
        "- ì¸ê°„ ìƒìŠ¹ ì²´í—˜ê³¼ì˜ ì—°ê²°\n"
        "- ì˜¤ëŠ˜ì˜ ì‹ ì•™ê³¼ ì‚¶ì— ì£¼ëŠ” êµí›ˆ\n\n"
        "2ë¶€. 5ì¥ ìŠ¬ë¼ì´ë“œ ê°œìš”\n"
        "ê° ìŠ¬ë¼ì´ë“œëŠ” ë‹¤ìŒ ìš”ì†Œë¥¼ í¬í•¨í•©ë‹ˆë‹¤:\n"
        "- ì œëª© 1ì¤„\n"
        "- í•µì‹¬ í¬ì¸íŠ¸ 3~5ê°œ\n"
        "- ë°œí‘œì ë…¸íŠ¸ (300~500ì)\n\n"
        "í˜•ì‹ ì˜ˆì‹œ:\n"
        "# ìŠ¬ë¼ì´ë“œ 1: ...\n"
        "- ...\n"
        "ë°œí‘œì ë…¸íŠ¸: ...\n\n"
        f"---\n\nì°¸ê³  ë³¸ë¬¸:\n{joined_passages}"
    )

    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” ìœ ë€ì‹œì•„ì„œ ì‹ í•™ê³¼ êµìœ¡ì— ëŠ¥í†µí•œ í•™ìì´ë‹¤."},
                {"role": "user", "content": prompt}
            ]
        )
        return resp.choices[0].message.content
    except Exception as e:
        return f"âš ï¸ GPT ì˜¤ë¥˜ ë°œìƒ: {e}"

# -----------------------
# UI
# -----------------------
st.subheader("1ï¸âƒ£ ì£¼ì œ ì…ë ¥")
term = st.text_input("ì˜ˆ: ì‹ ì„±, ìµœê·¹ì, ì¡°ìœ¨ì, ë¯¸ê°€ì—˜, ìƒìŠ¹, ì‹ ì•™", "", key="term_input")

st.subheader("2ï¸âƒ£ ê´€ë ¨ êµ¬ì ˆ ê²€ìƒ‰ ê²°ê³¼")

if not KR_PATH:
    st.error("ğŸ“‚ data í´ë”ì— urantia_ko.txt (ë˜ëŠ” urantia_kr.txt)ê°€ ì—†ìŠµë‹ˆë‹¤.")
else:
    if not urantia_lines:
        st.error("âš ï¸ íŒŒì¼ì€ ìˆì§€ë§Œ ë‚´ìš©ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì¸ì½”ë”©(UTF-8) í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    else:
        passages = search_passages(term, urantia_lines) if term else []
        if term and passages:
            for p in passages:
                st.markdown(p)
        elif term:
            st.info("ê´€ë ¨ êµ¬ì ˆì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë‹¨ì–´ë‚˜ ì£¼ì œë¥¼ ì…ë ¥í•´ë³´ì„¸ìš”.")

st.subheader("3ï¸âƒ£ AI ë³´ê³ ì„œ + ìŠ¬ë¼ì´ë“œ ìƒì„±")

if st.button("âœ¨ AI ë³´ê³ ì„œ ë° ìŠ¬ë¼ì´ë“œ ìƒì„±"):
    with st.spinner("AIê°€ ë³´ê³ ì„œë¥¼ ì‘ì„± ì¤‘ì…ë‹ˆë‹¤..."):
        passages = search_passages(term, urantia_lines)
        result = generate_gpt_report_and_slides(term, passages)
    st.markdown(result)
else:
    st.caption("ì£¼ì œë¥¼ ì…ë ¥í•˜ê³  ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ AIê°€ ë¶„ì„ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")

